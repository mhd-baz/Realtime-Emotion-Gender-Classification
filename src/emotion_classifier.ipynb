{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<br> Ayman FAHSI | A20440820\n",
        "\n",
        "Mouhammad BAZZI | A20522180\n",
        "\n",
        "\n",
        "CS577 - Fall 2022</br> <h1><br><b><font color='red'>Project</font></br></h1>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# <h1><b><font color='ORANGE'>EMOTION CLASSIFIER</h1>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## **LIBRAIRES IMPORTATIONS**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YokgibTvjfTI"
      },
      "outputs": [],
      "source": [
        "#Importing the libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras import Input, Model, layers\n",
        "from tensorflow.keras.layers import Conv2D, SeparableConv2D, BatchNormalization, MaxPooling2D, GlobalAveragePooling2D, AveragePooling2D, Activation, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## **DATA IMPORTATION & DATA PRE-PROCESSING**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "############################################################################################################\n",
        "\n",
        "#WE DEFINE THE CONSTANTS\n",
        "DATA_PATH = \"h:/Desktop/Deep Learning/cs577-f22-bazz-mouhammad/project/data/fer_emotion/train.csv\" #The path to the csv file\n",
        "VAL_RATIO = 0.2 #The ratio of the validation data\n",
        "TEST_RATIO = 0.2 #The ratio of the testing data\n",
        "RANDOM_STATE = 0 #The random state\n",
        "\n",
        "############################################################################################################"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### **DATA IMPORTATION**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#DATA IMPORTATION\n",
        "\n",
        "#The data is in the form of a csv file with the first column being the label and the rest of the columns being the pixels\n",
        "\n",
        "############################################################################################################\n",
        "\n",
        "#WE DEFINE THE CONSTANTS\n",
        "EMOTIONS = ['Angry', 'Disgust', 'Fear', 'Happy', 'Sad', 'Surprise', 'Neutral'] #The emotions that we are classifying\n",
        "\n",
        "\n",
        "############################################################################################################\n",
        "\n",
        "#Read the csv file using pandas\n",
        "data = pd.read_csv(DATA_PATH)\n",
        "\n",
        "#Split the data into features and labels\n",
        "x = data.iloc[:, 1:].values #The features\n",
        "y = data.iloc[:, 0].values #The labels\n",
        "\n",
        "#We have to split the features by taking the space as a delimiter and then we have to convert the string to float\n",
        "#in order to have an array of integers (pixels)\n",
        "# - Split the features by space\n",
        "x = [x[i][0].split(' ') for i in range(len(x))]\n",
        "# - Convert the string to float\n",
        "x = np.array(x, dtype='float32')\n",
        "\n",
        "#We have to reshape the features to 48x48\n",
        "x = x.reshape(x.shape[0], 48, 48, 1)  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### **DATA VISUALIZATION**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#VISUALIZATION OF THE DATA\n",
        "\n",
        "#We have to visualize the data in order to see if the data is balanced or not\n",
        "# - Visualize the data\n",
        "# - Plot the repartition of the emotions\n",
        "\n",
        "# - Visualize the data\n",
        "#Plot one image of each emotion\n",
        "fig, ax = plt.subplots(1, 7, figsize=(10, 10))\n",
        "for i in range(7):\n",
        "    ax[i].imshow(x[y==i][0].reshape(48, 48), cmap='gray')\n",
        "    ax[i].set_title(EMOTIONS[i])\n",
        "    #WE remove the ticks\n",
        "    ax[i].set_xticks([])\n",
        "    ax[i].set_yticks([])\n",
        "plt.show()\n",
        "\n",
        "# - Plot the repartition of the emotions\n",
        "plt.hist(y, bins=7)\n",
        "plt.title('Repartition of the emotions')\n",
        "plt.xlabel('Emotions')\n",
        "plt.ylabel('Number of images')\n",
        "plt.xticks(np.arange(7), EMOTIONS)\n",
        "#percentage of each emotion\n",
        "for i in range(7):\n",
        "    plt.text(x=i-0.1, y=np.sum(y==i)+100, s=str(np.round(np.sum(y==i)/len(y)*100, 2))+'%')\n",
        "plt.show()\n",
        "\n",
        "#We can see that the data is not balanced, we have more images of happy and very few images of disgust"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### **PRE-PROCESSING**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#DATA PREPROCESSING\n",
        "\n",
        "#We have to normalize the features and one hot encode the labels\n",
        "#We also have to split the data into training, testing and validation\n",
        "\n",
        "#Features normalization\n",
        "x = x/255.0\n",
        "\n",
        "#Labels one hot encoding\n",
        "y = to_categorical(y)\n",
        "\n",
        "#Split the data into training and testing\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=TEST_RATIO, random_state=RANDOM_STATE)\n",
        "\n",
        "#Split the training data into training and validation\n",
        "x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=VAL_RATIO, random_state=RANDOM_STATE)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## **MODEL ARCHITECTURE**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "############################################################################################################\n",
        "\n",
        "#WE DEFINE THE CONSTANTS\n",
        "REGULARIZATION = 0.01 #The regularization parameter\n",
        "LEARNING_RATE = 0.001 #The learning rate\n",
        "IMG_SIZE = 48 #The size of the images\n",
        "IMG_SHAPE = (IMG_SIZE, IMG_SIZE, 1) #The shape of the images\n",
        "OUTPUT_SHAPE = 7 #The shape of the output (number of classes)\n",
        "\n",
        "############################################################################################################\n",
        "\n",
        "# ------------------------------ #\n",
        "# ------------------------------ #\n",
        "# IMPORTANT:\n",
        "\n",
        "#YOU HAVE TO CHOOSE THE MODEL THAT YOU WANT TO USE BY EXECUTING ONE CELL AMONG THE NEXT 2 FOLLOWING CELLS\n",
        "\n",
        "# ------------------------------ #"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### <font color='SKY BLUE'>**FIRST MODEL - RESIDUAL MODEL**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#BUILDING THE MODEL\n",
        "\n",
        "#We define the input\n",
        "input = Input(shape=IMG_SHAPE)\n",
        "\n",
        "#We add twice: one convolutional layer with Batch Normalization\n",
        "x = Conv2D(9, (3, 3), strides=(1, 1), kernel_regularizer=tf.keras.regularizers.l2(REGULARIZATION), use_bias=False)(input)\n",
        "x = BatchNormalization()(x)\n",
        "x = Activation('relu')(x)\n",
        "x = Conv2D(9, (3, 3), strides=(1, 1), kernel_regularizer=tf.keras.regularizers.l2(REGULARIZATION), use_bias=False)(x)\n",
        "x = BatchNormalization()(x)\n",
        "x = Activation('relu')(x)\n",
        "\n",
        "#We will repeat the following 4 times:\n",
        "#-- Then we will do in parallel: one convolutional layer with Batch Normalization\n",
        "#-- and twice: one depth-wise seperable convolutional layer with Batch Normalization followed by a max pooling layer\n",
        "#-- then we will add the two branches\n",
        "\n",
        "#First repetition\n",
        "#The convolutional layer with Batch Normalization\n",
        "residual_1 = Conv2D(18, (1, 1), strides=(2, 2), padding='same', use_bias=False)(x)\n",
        "residual_1 = BatchNormalization()(residual_1)\n",
        "#The depth-wise seperable convolutional layer with Batch Normalization followed by a max pooling layer\n",
        "x1 = SeparableConv2D(18, (3, 3), padding='same', kernel_regularizer=tf.keras.regularizers.l2(REGULARIZATION), use_bias=False)(x)\n",
        "x1 = BatchNormalization()(x1)\n",
        "x1 = Activation('relu')(x1)\n",
        "x1 = SeparableConv2D(18, (3, 3), padding='same', kernel_regularizer=tf.keras.regularizers.l2(REGULARIZATION), use_bias=False)(x1)\n",
        "x1 = BatchNormalization()(x1)\n",
        "x1 = MaxPooling2D((3, 3), strides=(2, 2), padding='same')(x1)\n",
        "#We add the two branches\n",
        "x = layers.add([x1, residual_1])\n",
        "\n",
        "#Second repetition\n",
        "#The convolutional layer with Batch Normalization\n",
        "residual_2 = Conv2D(36, (1, 1), strides=(2, 2), padding='same', use_bias=False)(x)\n",
        "residual_2 = BatchNormalization()(residual_2)\n",
        "#The depth-wise seperable convolutional layer with Batch Normalization followed by a max pooling layer\n",
        "x2 = SeparableConv2D(36, (3, 3), padding='same', kernel_regularizer=tf.keras.regularizers.l2(REGULARIZATION), use_bias=False)(x)\n",
        "x2 = BatchNormalization()(x2)\n",
        "x2 = Activation('relu')(x2)\n",
        "x2 = SeparableConv2D(36, (3, 3), padding='same', kernel_regularizer=tf.keras.regularizers.l2(REGULARIZATION), use_bias=False)(x2)\n",
        "x2 = BatchNormalization()(x2)\n",
        "x2 = MaxPooling2D((3, 3), strides=(2, 2), padding='same')(x2)\n",
        "#We add the two branches\n",
        "x = layers.add([x2, residual_2])\n",
        "\n",
        "#Third repetition\n",
        "#The convolutional layer with Batch Normalization\n",
        "residual_3 = Conv2D(72, (1, 1), strides=(2, 2), padding='same', use_bias=False)(x)\n",
        "residual_3 = BatchNormalization()(residual_3)\n",
        "#The depth-wise seperable convolutional layer with Batch Normalization followed by a max pooling layer\n",
        "x3 = SeparableConv2D(72, (3, 3), padding='same', kernel_regularizer=tf.keras.regularizers.l2(REGULARIZATION), use_bias=False)(x)\n",
        "x3 = BatchNormalization()(x3)\n",
        "x3 = Activation('relu')(x3)\n",
        "x3 = SeparableConv2D(72, (3, 3), padding='same', kernel_regularizer=tf.keras.regularizers.l2(REGULARIZATION), use_bias=False)(x3)\n",
        "x3 = BatchNormalization()(x3)\n",
        "x3 = MaxPooling2D((3, 3), strides=(2, 2), padding='same')(x3)\n",
        "#We add the two branches\n",
        "x = layers.add([x3, residual_3])\n",
        "\n",
        "\n",
        "#Fourth repetition\n",
        "#The convolutional layer with Batch Normalization\n",
        "residual_4 = Conv2D(144, (1, 1), strides=(2, 2), padding='same', use_bias=False)(x)\n",
        "residual_4 = BatchNormalization()(residual_4)\n",
        "#The depth-wise seperable convolutional layer with Batch Normalization followed by a max pooling layer\n",
        "x4 = SeparableConv2D(144, (3, 3), padding='same', kernel_regularizer=tf.keras.regularizers.l2(REGULARIZATION), use_bias=False)(x)\n",
        "x4 = BatchNormalization()(x4)\n",
        "x4 = Activation('relu')(x4)\n",
        "x4 = SeparableConv2D(144, (3, 3), padding='same', kernel_regularizer=tf.keras.regularizers.l2(REGULARIZATION), use_bias=False)(x4)\n",
        "x4 = BatchNormalization()(x4)\n",
        "x4 = MaxPooling2D((3, 3), strides=(2, 2), padding='same')(x4)\n",
        "#We add the two branches\n",
        "x = layers.add([x4, residual_4])\n",
        "\n",
        "\n",
        "#Now we add a convolutional layer then a global average pooling layer and then a softmax layer\n",
        "x = Conv2D(OUTPUT_SHAPE, (3, 3), padding='same')(x)\n",
        "x = GlobalAveragePooling2D()(x)\n",
        "output = Activation('softmax')(x)\n",
        "\n",
        "#We define the model\n",
        "model = Model(input, output)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### <font color='SKY BLUE'> **SECOND MODEL - BASIC CNN MODEL**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#BUILDING THE MODEL\n",
        "#We will use the Sequential API to build the model\n",
        "\n",
        "basic_model = Sequential()\n",
        "\n",
        "# Layer 1\n",
        "basic_model.add(Conv2D(16, kernel_size=(7, 7), padding='same', input_shape=IMG_SHAPE))\n",
        "basic_model.add(BatchNormalization())\n",
        "basic_model.add(Activation('relu'))\n",
        "basic_model.add(Conv2D(16, kernel_size=(7, 7), padding='same'))\n",
        "basic_model.add(BatchNormalization())\n",
        "basic_model.add(Activation('relu'))\n",
        "basic_model.add(AveragePooling2D(pool_size=(2, 2), padding='same'))\n",
        "basic_model.add(Dropout(.5))\n",
        "\n",
        "# Layer 2\n",
        "basic_model.add(Conv2D(32, kernel_size=(5, 5), padding='same'))\n",
        "basic_model.add(BatchNormalization())\n",
        "basic_model.add(Conv2D(32, kernel_size=(5, 5), padding='same'))\n",
        "basic_model.add(BatchNormalization())\n",
        "basic_model.add(Activation('relu'))\n",
        "basic_model.add(AveragePooling2D(pool_size=(2, 2), padding='same'))\n",
        "basic_model.add(Dropout(.5))\n",
        "\n",
        "# Layer 3\n",
        "basic_model.add(Conv2D(64, kernel_size=(3, 3), padding='same'))\n",
        "basic_model.add(BatchNormalization())\n",
        "basic_model.add(Conv2D(64, kernel_size=(3, 3), padding='same'))\n",
        "basic_model.add(BatchNormalization())\n",
        "basic_model.add(Activation('relu'))\n",
        "basic_model.add(AveragePooling2D(pool_size=(2, 2), padding='same'))\n",
        "basic_model.add(Dropout(.5))\n",
        "\n",
        "# Layer 4\n",
        "basic_model.add(Conv2D(128, kernel_size=(3, 3), padding='same'))\n",
        "basic_model.add(BatchNormalization())\n",
        "basic_model.add(Conv2D(128, kernel_size=(3, 3), padding='same'))\n",
        "basic_model.add(BatchNormalization())\n",
        "basic_model.add(Activation('relu'))\n",
        "basic_model.add(AveragePooling2D(pool_size=(2, 2), padding='same'))\n",
        "basic_model.add(Dropout(.5))\n",
        "\n",
        "# Output\n",
        "basic_model.add(Conv2D(256, kernel_size=(3, 3), padding='same'))\n",
        "basic_model.add(BatchNormalization())\n",
        "basic_model.add(Conv2D(OUTPUT_SHAPE, kernel_size=(3, 3), padding='same'))\n",
        "basic_model.add(GlobalAveragePooling2D())\n",
        "basic_model.add(Activation('softmax'))\n",
        "\n",
        "#We render basic_model into model\n",
        "model = basic_model\n",
        "\n",
        "model.summary()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## **COMPILE MODEL**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#We compile the model\n",
        "\n",
        "model.compile(optimizer=Adam(learning_rate=LEARNING_RATE), loss='categorical_crossentropy', metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## **MODEL TRAINING**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "############################################################################################################\n",
        "\n",
        "#WE DEFINE THE CONSTANTS\n",
        "EPOCHS = 3 #Number of epochs\n",
        "BATCH_SIZE = 32 #Batch size\n",
        "WITH_DATA_AUGMENTATION = False #Use of data augmentation or not\n",
        "\n",
        "# ------------------------------ #\n",
        "#SPECIFIC FOR THE DATA AUGMENTATION\n",
        "# ------------------------------ #\n",
        "\n",
        "#We define the data augmentation\n",
        "data_augmentation = ImageDataGenerator(\n",
        "    rotation_range=15,\n",
        "    width_shift_range=0.1,\n",
        "    height_shift_range=0.1,\n",
        "    shear_range=0.15,\n",
        "    zoom_range=0.15,\n",
        "    horizontal_flip=True,\n",
        "    fill_mode='nearest')\n",
        "\n",
        "# ------------------------------ #\n",
        "# ------------------------------ #\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#TRAINING THE MODEL\n",
        "\n",
        "#We train the model\n",
        "if WITH_DATA_AUGMENTATION:\n",
        "    train_generator = data_augmentation.flow(x_train, y_train, batch_size=BATCH_SIZE)\n",
        "    steps_per_epoch = train_generator.n // train_generator.batch_size\n",
        "    history = model.fit(train_generator, epochs=EPOCHS, steps_per_epoch=steps_per_epoch, validation_data=(x_val, y_val))\n",
        "else:\n",
        "    history = model.fit(x_train, y_train, batch_size=BATCH_SIZE, epochs=EPOCHS, validation_data=(x_val, y_val))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#PLOTTING THE RESULTS\n",
        "\n",
        "#We plot the results\n",
        "plt.figure(figsize=(15, 5))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
        "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
        "plt.legend(loc='lower right')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.ylim([min(plt.ylim()),1])\n",
        "plt.title('Training and Validation Accuracy')\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history.history['loss'], label='Training Loss')\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "plt.legend(loc='upper right')\n",
        "plt.ylabel('Cross Entropy')\n",
        "plt.ylim([0,1.0])\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## **MODEL EVALUATION**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "############################################################################################################\n",
        "\n",
        "#WE DEFINE THE CONSTANTS\n",
        "IDEAL_EPOCH = 1 #We choose the ideal epoch (thanks to the plot above)\n",
        "\n",
        "############################################################################################################"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#EVALUATING THE MODEL\n",
        "\n",
        "# - First we have to train again the model with the whole training set (train + validation)\n",
        "\n",
        "x_train_val = np.concatenate((x_train, x_val), axis=0)\n",
        "y_train_val = np.concatenate((y_train, y_val), axis=0)\n",
        "\n",
        "if WITH_DATA_AUGMENTATION:\n",
        "    train_generator = data_augmentation.flow(x_train_val, y_train_val, batch_size=BATCH_SIZE)\n",
        "    steps_per_epoch = train_generator.n // train_generator.batch_size\n",
        "    history = model.fit(train_generator, epochs=IDEAL_EPOCH, steps_per_epoch=steps_per_epoch)\n",
        "else:\n",
        "    history = model.fit(x_train_val, y_train_val, batch_size=BATCH_SIZE, epochs=IDEAL_EPOCH, verbose=0)\n",
        "\n",
        "\n",
        "# - Then we evaluate the model on the test set\n",
        "\n",
        "score = model.evaluate(x_test, y_test, verbose=0)\n",
        "print('Test loss:', score[0])\n",
        "print('Test accuracy:', score[1])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#CONFUSION MATRIX\n",
        "\n",
        "EMOTIONS = ['Angry', 'Disgust', 'Fear', 'Happy', 'Sad', 'Surprise', 'Neutral']\n",
        "\n",
        "#We compute the predictions\n",
        "y_pred = model.predict(x_test)\n",
        "y_pred = np.argmax(y_pred, axis=1)\n",
        "\n",
        "#We store the true labels\n",
        "y_true = np.argmax(y_test, axis=1)\n",
        "\n",
        "#We compute the confusion matrix\n",
        "cm = confusion_matrix(y_true, y_pred)\n",
        "\n",
        "#We plot the confusion matrix\n",
        "df_cm = pd.DataFrame(cm, index = [i for i in EMOTIONS],\n",
        "                    columns = [i for i in EMOTIONS])\n",
        "plt.figure(figsize = (10,7))\n",
        "sns.heatmap(df_cm, annot=True, fmt='g')\n",
        "plt.title('Confusion matrix')\n",
        "plt.xlabel('Predicted label')\n",
        "plt.ylabel('True label')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## **WE SAVE THE MODEL**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yu7gzQkE3boI"
      },
      "outputs": [],
      "source": [
        "#We save the model\n",
        "\n",
        "############################################################################################################\n",
        "#WE DEFINE THE CONSTANTS\n",
        "name = 'model' #Name of the model\n",
        "\n",
        "############################################################################################################\n",
        "\n",
        "\n",
        "\n",
        "data_augmented = 'data_augmented' if WITH_DATA_AUGMENTATION else ''\n",
        "\n",
        "model.save(name + '_' + data_augmented + '.h5')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## **WE LOAD A MODEL**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#We load the model\n",
        "\n",
        "############################################################################################################\n",
        "#WE DEFINE THE CONSTANTS\n",
        "file_name = 'model' #Name of the file\n",
        "\n",
        "############################################################################################################\n",
        "\n",
        "model = load_model(file_name + '.h5')\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "cbkPsj4Sg-UL",
        "fmvwEe4wm-V7",
        "dpt2R3k9nMaN",
        "UphGzo2nOe5h",
        "D1c-q8FI_DY6",
        "_gAH7lH4AlsO"
      ],
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3.8.3 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    },
    "vscode": {
      "interpreter": {
        "hash": "570feb405e2e27c949193ac68f46852414290d515b0ba6e5d90d076ed2284471"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
