{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br> Ayman FAHSI | A20440820\n",
    "\n",
    "Mouhammad BAZZI | A20522180\n",
    "\n",
    "\n",
    "CS577 - Fall 2022</br> <h1><br><b><font color='red'>Project</font></br></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *Credit for the DataBase creators*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The following is a credit that the creator of the database that we use require us to put in our code in order to use theire database:**\n",
    "\n",
    "\n",
    "@article\n",
    "\n",
    "{Rothe-IJCV-2018,\n",
    "\n",
    "  author = {Rasmus Rothe and Radu Timofte and Luc Van Gool},\n",
    "\n",
    "  title = {Deep expectation of real and apparent age from a single image without facial landmarks},\n",
    "\n",
    "  journal = {International Journal of Computer Vision},\n",
    "\n",
    "  volume={126},\n",
    "\n",
    "  number={2-4},\n",
    "\n",
    "  pages={144--157},\n",
    "\n",
    "  year={2018},\n",
    "\n",
    "  publisher={Springer}\n",
    "\n",
    "}\n",
    "\n",
    "\n",
    "@InProceedings\n",
    "\n",
    "{Rothe-ICCVW-2015,\n",
    "\n",
    "  author = {Rasmus Rothe and Radu Timofte and Luc Van Gool},\n",
    "\n",
    "  title = {DEX: Deep EXpectation of apparent age from a single image},\n",
    "\n",
    "  booktitle = {IEEE International Conference on Computer Vision Workshops (ICCVW)},\n",
    "\n",
    "  year = {2015},\n",
    "\n",
    "  month = {December},\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <h1><b><font color='ORANGE'>GENDER CLASSIFIER</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **LIBRAIRES IMPORTATIONS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing the libraries\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.io\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import cv2\n",
    "\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras import Input, Model, layers\n",
    "from tensorflow.keras.layers import Conv2D, SeparableConv2D, BatchNormalization, MaxPooling2D, GlobalAveragePooling2D, AveragePooling2D, Activation, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **DATA IMPORTATION & DATA PRE-PROCESSING**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################################################################################\n",
    "\n",
    "#WE DEFINE THE CONSTANTS\n",
    "META_DATA_PATH = 'h:/Desktop/Deep Learning/cs577-f22-bazz-mouhammad/project/data/imdb_meta/imdb/imdb.mat' #Path to the meta data\n",
    "IMAGES_PATH = 'h:/Desktop/Deep Learning/cs577-f22-bazz-mouhammad/project/data/imdb_crop/' #Path to the images\n",
    "IMG_SIZE = 64 #Size of the images\n",
    "\n",
    "TEST_RATIO = 0.2 #Ratio of the test set\n",
    "VAL_RATIO = 0.2 #Ratio of the validation set\n",
    "RANDOM_STATE = 42 #Random state for the train_test_split function\n",
    "\n",
    "NUMBER_IMDB_IMAGES = 100000 #Number of images in the imdb dataset\n",
    "\n",
    "\n",
    "############################################################################################################\n",
    "############################################################################################################\n",
    "\n",
    "# ------------------------------ #\n",
    "#SPECIFIC FOR OUR PROBLEM OF DATA IMPORTATION\n",
    "# ------------------------------ #\n",
    "\n",
    "PROBLEM_WITH_DATA_IMPORTATION = False\n",
    "\n",
    "if PROBLEM_WITH_DATA_IMPORTATION:\n",
    "    #We define only the folders we can use\n",
    "    PATH_BEGINNING = [str(i)+str(j) for i in range(2) for j in range(10)]\n",
    "    PATH_BEGINNING.append('20')\n",
    "    PATH_BEGINNING.append('21')\n",
    "    PATH_BEGINNING.append('22')\n",
    "    PATH_BEGINNING.append('23')\n",
    "    PATH_BEGINNING.append('24')\n",
    "    PATH_BEGINNING.append('25')\n",
    "    PATH_BEGINNING.append('26')\n",
    "    PATH_BEGINNING.append('27')\n",
    "    PATH_BEGINNING.append('28')\n",
    "\n",
    "    #We define again the NUMBER_IMDB_IMAGES because we can only use a subset of the dataset\n",
    "    NUMBER_IMDB_IMAGES = 130000\n",
    "else:\n",
    "    PATH_BEGINNING = [str(i)+str(j) for i in range(10) for j in range(10)]\n",
    "\n",
    "# ------------------------------ #\n",
    "# ------------------------------ #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *Importing the Meta Data*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We want to read the metadata file that is in .mat (MatLab) in order to know the labels of the images\n",
    "\n",
    "#We define all the names of the different categories of the metadata of the images\n",
    "CATEGORIES = [\"date_of_birth\", \"photo_taken\", \"full_path\", \"gender\", \"name\", \"face_location\", \"face_score\", \"second_face_score\", \"celeb_names\", \"celeb_id\"]\n",
    "GENDER = [\"FEMALE\", \"MALE\"]\n",
    "\n",
    "#We read the metadata file\n",
    "mat = scipy.io.loadmat(META_DATA_PATH)\n",
    "\n",
    "#We get only the part that we care about\n",
    "meta_data = mat['imdb'][0,0]\n",
    "\n",
    "#We will collect only the full_path (id) and the gender (label) of the images\n",
    "#We create a list of tuples (id, label)\n",
    "data = []\n",
    "i = 0\n",
    "#We iterate over all the images\n",
    "for i in range(NUMBER_IMDB_IMAGES):\n",
    "    #We only take the images that are in the folders we can use\n",
    "    if (meta_data[2][0][i][0][0]+meta_data[2][0][i][0][1]) in PATH_BEGINNING:\n",
    "      data.append((meta_data[2][0][i][0], meta_data[3][0][i]))\n",
    "\n",
    "\n",
    "\n",
    "#ATTENTION: We do not have necessarily NUMBER_IMDB_IMAGES images because some of them are not in the folders we can use\n",
    "print(\"Number of images in the dataset: \", len(data))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *Importing the Images using the meta data*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing the images (ATTENTION: This takes a while)\n",
    "\n",
    "#We create a list of images\n",
    "images = []\n",
    "#We create a list of labels\n",
    "labels = []\n",
    "#We initialize the number of images we have seen\n",
    "number_images = 0\n",
    "\n",
    "#We iterate over all the images\n",
    "for path, label in data:\n",
    "\n",
    "    if number_images % 10000 == 0:\n",
    "        print(\"Number of images we have seen: \", number_images)\n",
    "\n",
    "    #we will select only the images that have a non corrupted label (0 or 1)\n",
    "    if (label == 0) or (label == 1):\n",
    "        try:\n",
    "            #We read the image\n",
    "            image = cv2.imread(IMAGES_PATH+path)\n",
    "            #We resize the image\n",
    "            image = cv2.resize(image, (IMG_SIZE, IMG_SIZE))\n",
    "            #We add the image to the list of images\n",
    "            images.append(image)    \n",
    "            #We add the label to the list of labels\n",
    "            labels.append(label)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    #We increment the number of images we have seen\n",
    "    number_images += 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Data Visualization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#VISUALIZATION OF THE DATA\n",
    "\n",
    "GENDER = [\"FEMALE\", \"MALE\"]\n",
    "\n",
    "#We have to visualize the data in order to see if the data is balanced or not\n",
    "# - Visualize the data\n",
    "# - Plot the Gender repartition\n",
    "\n",
    "# - Visualize the data\n",
    "#Plot one image of each gender\n",
    "#fig, ax = plt.subplots(1, 2, figsize=(10, 5))\n",
    "#for i in range(2):\n",
    "#    ax[i].imshow(images[labels.index(i)].reshape(IMG_SIZE, IMG_SIZE, 3))\n",
    "#    ax[i].set_title(GENDER[i])\n",
    "    #We remove the ticks\n",
    "#    ax[i].set_xticks([])\n",
    "#    ax[i].set_yticks([])\n",
    "#plt.show()\n",
    "\n",
    "# - Plot the Gender repartition\n",
    "#We plot the repartition \n",
    "#We create a list of the number of images for each label\n",
    "number_images = [labels.count(0), labels.count(1)]\n",
    "#We create a list of the percentage of images for each label\n",
    "percentage_images = [labels.count(0)/len(labels), labels.count(1)/len(labels)]\n",
    "#We create a figure\n",
    "fig, ax = plt.subplots()\n",
    "#We create a bar plot\n",
    "ax.bar(GENDER, number_images)\n",
    "#We add the percentage of images for each label\n",
    "for i in range(2):\n",
    "    ax.text(GENDER[i], number_images[i], str(round(percentage_images[i]*100, 2))+'%', ha='center', va='bottom')\n",
    "#We add the title\n",
    "ax.set_title('Repartition of the labels')\n",
    "#We add the x label\n",
    "ax.set_xlabel('Labels')\n",
    "#We add the y label\n",
    "ax.set_ylabel('Number of images')\n",
    "#We show the plot\n",
    "plt.show()\n",
    "\n",
    "#We display the number of images we have in total\n",
    "print(\"Number of images in the dataset: \", len(images))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Balance the class repartition** (optional pre-processing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#BALANCING THE DATA (OPTIONAL) #WILL DELETE SOME IMAGES FROM A SPECIFIC CLASS\n",
    "\n",
    "#We get the number of images for each label\n",
    "number_images = [labels.count(0), labels.count(1)] \n",
    "#We get the index of the label with the most images\n",
    "index_max = np.argmax(number_images) \n",
    "\n",
    "\n",
    "# - We balance the data by removing images of the label with the most images\n",
    "\n",
    "#We get the indexes of the images of the class with the most images\n",
    "indexes = [j for j, x in enumerate(labels) if x == index_max]\n",
    "#We shuffle the indexes\n",
    "random.shuffle(indexes)\n",
    "#We truncate the list of images and labels\n",
    "images_trunc = [images[j] for j in indexes[:number_images[1-index_max]]]\n",
    "labels_trunc = [labels[j] for j in indexes[:number_images[1-index_max]]]\n",
    "\n",
    "\n",
    "# - We balance the data by duplicating images of the label with the least images\n",
    "\n",
    "#We get the indexes of the images of the class with the least images\n",
    "indexes = [j for j, x in enumerate(labels) if x == (1-index_max)]\n",
    "#We add the images of the class with the least images\n",
    "images_trunc += [images[j] for j in indexes]\n",
    "labels_trunc += [labels[j] for j in indexes]\n",
    "\n",
    "\n",
    "\n",
    "#We shuffle the images and labels\n",
    "c = list(zip(images_trunc, labels_trunc))\n",
    "random.shuffle(c)\n",
    "images_trunc, labels_trunc = zip(*c)\n",
    "\n",
    "\n",
    "# - Plot the Gender repartition\n",
    "#We create a list of the number of images for each label\n",
    "number_images = [labels_trunc.count(0), labels_trunc.count(1)]\n",
    "#We create a list of the percentage of images for each label\n",
    "percentage_images = [labels_trunc.count(0)/len(labels_trunc), labels_trunc.count(1)/len(labels_trunc)]\n",
    "#We create a figure\n",
    "fig, ax = plt.subplots()\n",
    "#We create a bar plot\n",
    "ax.bar(GENDER, number_images)\n",
    "#We add the percentage of images for each label\n",
    "for i in range(2):\n",
    "    ax.text(GENDER[i], number_images[i], str(round(percentage_images[i]*100, 2))+'%', ha='center', va='bottom')\n",
    "#We add the title\n",
    "ax.set_title('Repartition of the labels')\n",
    "#We add the x label\n",
    "ax.set_xlabel('Labels')\n",
    "#We add the y label\n",
    "ax.set_ylabel('Number of images')\n",
    "#We show the plot\n",
    "plt.show()\n",
    "\n",
    "\n",
    "#We define the images and labels as images_trunc and labels_trunc\n",
    "images = images_trunc\n",
    "labels = labels_trunc\n",
    "\n",
    "#We display the number of images we have in total after balancing the data\n",
    "print(\"Number of images in the dataset: \", len(images))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **PRE-PROCESSING**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DATA PREPROCESSING\n",
    "\n",
    "#We convert the list of images to a numpy array\n",
    "images = np.array(images)\n",
    "#We convert the list of labels to a numpy array\n",
    "labels = np.array(labels)\n",
    "\n",
    "#We split the data into train, test and validation\n",
    "x_train, x_test, y_train, y_test = train_test_split(images, labels, test_size=TEST_RATIO, random_state=RANDOM_STATE)\n",
    "x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=VAL_RATIO, random_state=RANDOM_STATE)\n",
    "\n",
    "#We normalize the data\n",
    "x_train = x_train/255\n",
    "x_test = x_test/255\n",
    "x_val = x_val/255\n",
    "\n",
    "#We one-hot encode the labels\n",
    "y_train = to_categorical(y_train)\n",
    "y_test = to_categorical(y_test)\n",
    "y_val = to_categorical(y_val)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **MODEL ARCHITECTURE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################################################################################\n",
    "\n",
    "#WE DEFINE THE CONSTANTS\n",
    "REGULARIZATION = 0.01 #The regularization parameter\n",
    "LEARNING_RATE = 0.001 #The learning rate\n",
    "IMG_SIZE = 64 #The size of the images\n",
    "IMG_SHAPE = (IMG_SIZE, IMG_SIZE, 3) #The shape of the images\n",
    "OUTPUT_SHAPE = 2 #The shape of the output (number of classes)\n",
    "\n",
    "############################################################################################################\n",
    "\n",
    "# ------------------------------ #\n",
    "# ------------------------------ #\n",
    "# IMPORTANT:\n",
    "\n",
    "#YOU HAVE TO CHOOSE THE MODEL THAT YOU WANT TO USE BY EXECUTING ONE CELL AMONG THE NEXT 2 FOLLOWING CELLS\n",
    "\n",
    "# ------------------------------ #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='SKY BLUE'>**FIRST MODEL - RESIDUAL MODEL**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#BUILDING THE MODEL\n",
    "\n",
    "#We define the input\n",
    "input = Input(shape=IMG_SHAPE)\n",
    "\n",
    "#We add twice: one convolutional layer with Batch Normalization\n",
    "x = Conv2D(9, (3, 3), strides=(1, 1), kernel_regularizer=tf.keras.regularizers.l2(REGULARIZATION), use_bias=False)(input)\n",
    "x = BatchNormalization()(x)\n",
    "x = Activation('relu')(x)\n",
    "x = Conv2D(9, (3, 3), strides=(1, 1), kernel_regularizer=tf.keras.regularizers.l2(REGULARIZATION), use_bias=False)(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Activation('relu')(x)\n",
    "\n",
    "#We will repeat the following 4 times:\n",
    "#-- Then we will do in parallel: one convolutional layer with Batch Normalization\n",
    "#-- and twice: one depth-wise seperable convolutional layer with Batch Normalization followed by a max pooling layer\n",
    "#-- then we will add the two branches\n",
    "\n",
    "#First repetition\n",
    "#The convolutional layer with Batch Normalization\n",
    "residual_1 = Conv2D(18, (1, 1), strides=(2, 2), padding='same', use_bias=False)(x)\n",
    "residual_1 = BatchNormalization()(residual_1)\n",
    "#The depth-wise seperable convolutional layer with Batch Normalization followed by a max pooling layer\n",
    "x1 = SeparableConv2D(18, (3, 3), padding='same', kernel_regularizer=tf.keras.regularizers.l2(REGULARIZATION), use_bias=False)(x)\n",
    "x1 = BatchNormalization()(x1)\n",
    "x1 = Activation('relu')(x1)\n",
    "x1 = SeparableConv2D(18, (3, 3), padding='same', kernel_regularizer=tf.keras.regularizers.l2(REGULARIZATION), use_bias=False)(x1)\n",
    "x1 = BatchNormalization()(x1)\n",
    "x1 = MaxPooling2D((3, 3), strides=(2, 2), padding='same')(x1)\n",
    "#We add the two branches\n",
    "x = layers.add([x1, residual_1])\n",
    "\n",
    "#Second repetition\n",
    "#The convolutional layer with Batch Normalization\n",
    "residual_2 = Conv2D(36, (1, 1), strides=(2, 2), padding='same', use_bias=False)(x)\n",
    "residual_2 = BatchNormalization()(residual_2)\n",
    "#The depth-wise seperable convolutional layer with Batch Normalization followed by a max pooling layer\n",
    "x2 = SeparableConv2D(36, (3, 3), padding='same', kernel_regularizer=tf.keras.regularizers.l2(REGULARIZATION), use_bias=False)(x)\n",
    "x2 = BatchNormalization()(x2)\n",
    "x2 = Activation('relu')(x2)\n",
    "x2 = SeparableConv2D(36, (3, 3), padding='same', kernel_regularizer=tf.keras.regularizers.l2(REGULARIZATION), use_bias=False)(x2)\n",
    "x2 = BatchNormalization()(x2)\n",
    "x2 = MaxPooling2D((3, 3), strides=(2, 2), padding='same')(x2)\n",
    "#We add the two branches\n",
    "x = layers.add([x2, residual_2])\n",
    "\n",
    "#Third repetition\n",
    "#The convolutional layer with Batch Normalization\n",
    "residual_3 = Conv2D(72, (1, 1), strides=(2, 2), padding='same', use_bias=False)(x)\n",
    "residual_3 = BatchNormalization()(residual_3)\n",
    "#The depth-wise seperable convolutional layer with Batch Normalization followed by a max pooling layer\n",
    "x3 = SeparableConv2D(72, (3, 3), padding='same', kernel_regularizer=tf.keras.regularizers.l2(REGULARIZATION), use_bias=False)(x)\n",
    "x3 = BatchNormalization()(x3)\n",
    "x3 = Activation('relu')(x3)\n",
    "x3 = SeparableConv2D(72, (3, 3), padding='same', kernel_regularizer=tf.keras.regularizers.l2(REGULARIZATION), use_bias=False)(x3)\n",
    "x3 = BatchNormalization()(x3)\n",
    "x3 = MaxPooling2D((3, 3), strides=(2, 2), padding='same')(x3)\n",
    "#We add the two branches\n",
    "x = layers.add([x3, residual_3])\n",
    "\n",
    "\n",
    "#Fourth repetition\n",
    "#The convolutional layer with Batch Normalization\n",
    "residual_4 = Conv2D(144, (1, 1), strides=(2, 2), padding='same', use_bias=False)(x)\n",
    "residual_4 = BatchNormalization()(residual_4)\n",
    "#The depth-wise seperable convolutional layer with Batch Normalization followed by a max pooling layer\n",
    "x4 = SeparableConv2D(144, (3, 3), padding='same', kernel_regularizer=tf.keras.regularizers.l2(REGULARIZATION), use_bias=False)(x)\n",
    "x4 = BatchNormalization()(x4)\n",
    "x4 = Activation('relu')(x4)\n",
    "x4 = SeparableConv2D(144, (3, 3), padding='same', kernel_regularizer=tf.keras.regularizers.l2(REGULARIZATION), use_bias=False)(x4)\n",
    "x4 = BatchNormalization()(x4)\n",
    "x4 = MaxPooling2D((3, 3), strides=(2, 2), padding='same')(x4)\n",
    "#We add the two branches\n",
    "x = layers.add([x4, residual_4])\n",
    "\n",
    "\n",
    "#Now we add a convolutional layer then a global average pooling layer and then a softmax layer\n",
    "x = Conv2D(OUTPUT_SHAPE, (3, 3))(x)\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "output = Activation('softmax')(x)\n",
    "\n",
    "#We define the model\n",
    "model = Model(input, output)\n",
    "\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='SKY BLUE'> **SECOND MODEL - BASIC CNN MODEL**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#BUILDING THE MODEL\n",
    "#We will use the Sequential API to build the model\n",
    "\n",
    "basic_model = Sequential()\n",
    "\n",
    "# Layer 1\n",
    "basic_model.add(Conv2D(16, kernel_size=(7, 7), padding='same', input_shape=IMG_SHAPE))\n",
    "basic_model.add(BatchNormalization())\n",
    "basic_model.add(Activation('relu'))\n",
    "basic_model.add(Conv2D(16, kernel_size=(7, 7), padding='same'))\n",
    "basic_model.add(BatchNormalization())\n",
    "basic_model.add(Activation('relu'))\n",
    "basic_model.add(AveragePooling2D(pool_size=(2, 2), padding='same'))\n",
    "basic_model.add(Dropout(.5))\n",
    "\n",
    "# Layer 2\n",
    "basic_model.add(Conv2D(32, kernel_size=(5, 5), padding='same'))\n",
    "basic_model.add(BatchNormalization())\n",
    "basic_model.add(Conv2D(32, kernel_size=(5, 5), padding='same'))\n",
    "basic_model.add(BatchNormalization())\n",
    "basic_model.add(Activation('relu'))\n",
    "basic_model.add(AveragePooling2D(pool_size=(2, 2), padding='same'))\n",
    "basic_model.add(Dropout(.5))\n",
    "\n",
    "# Layer 3\n",
    "basic_model.add(Conv2D(64, kernel_size=(3, 3), padding='same'))\n",
    "basic_model.add(BatchNormalization())\n",
    "basic_model.add(Conv2D(64, kernel_size=(3, 3), padding='same'))\n",
    "basic_model.add(BatchNormalization())\n",
    "basic_model.add(Activation('relu'))\n",
    "basic_model.add(AveragePooling2D(pool_size=(2, 2), padding='same'))\n",
    "basic_model.add(Dropout(.5))\n",
    "\n",
    "# Layer 4\n",
    "basic_model.add(Conv2D(128, kernel_size=(3, 3), padding='same'))\n",
    "basic_model.add(BatchNormalization())\n",
    "basic_model.add(Conv2D(128, kernel_size=(3, 3), padding='same'))\n",
    "basic_model.add(BatchNormalization())\n",
    "basic_model.add(Activation('relu'))\n",
    "basic_model.add(AveragePooling2D(pool_size=(2, 2), padding='same'))\n",
    "basic_model.add(Dropout(.5))\n",
    "\n",
    "# Output\n",
    "basic_model.add(Conv2D(256, kernel_size=(3, 3), padding='same'))\n",
    "basic_model.add(BatchNormalization())\n",
    "basic_model.add(Conv2D(OUTPUT_SHAPE, kernel_size=(3, 3), padding='same'))\n",
    "basic_model.add(GlobalAveragePooling2D())\n",
    "basic_model.add(Activation('softmax'))\n",
    "\n",
    "#We render basic_model into model\n",
    "model = basic_model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **COMPILE MODEL**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We compile the model\n",
    "\n",
    "model.compile(optimizer=Adam(learning_rate=LEARNING_RATE), loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **MODEL TRAINING**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################################################################################\n",
    "\n",
    "#WE DEFINE THE CONSTANTS\n",
    "EPOCHS = 10 #Number of epochs\n",
    "BATCH_SIZE = 32 #Batch size\n",
    "WITH_DATA_AUGMENTATION = False #Use of data augmentation or not\n",
    "\n",
    "# ------------------------------ #\n",
    "#SPECIFIC FOR THE DATA AUGMENTATION\n",
    "# ------------------------------ #\n",
    "\n",
    "#We define the data augmentation\n",
    "data_augmentation = ImageDataGenerator(\n",
    "    rotation_range=15,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    shear_range=0.15,\n",
    "    zoom_range=0.15,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest')\n",
    "\n",
    "# ------------------------------ #\n",
    "# ------------------------------ #\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TRAINING THE MODEL\n",
    "\n",
    "#We train the model\n",
    "if WITH_DATA_AUGMENTATION:\n",
    "    train_generator = data_augmentation.flow(x_train, y_train, batch_size=BATCH_SIZE)\n",
    "    steps_per_epoch = train_generator.n // train_generator.batch_size\n",
    "    history = model.fit(train_generator, epochs=EPOCHS, steps_per_epoch=steps_per_epoch, validation_data=(x_val, y_val))\n",
    "else:\n",
    "    history = model.fit(x_train, y_train, batch_size=BATCH_SIZE, epochs=EPOCHS, validation_data=(x_val, y_val))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PLOTTING THE RESULTS\n",
    "\n",
    "#We plot the results\n",
    "plt.figure(figsize=(15, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.ylim([min(plt.ylim()),1])\n",
    "plt.title('Training and Validation Accuracy')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.ylabel('Cross Entropy')\n",
    "plt.ylim([0,1.0])\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **MODEL EVALUATION**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################################################################################\n",
    "\n",
    "#WE DEFINE THE CONSTANTS\n",
    "IDEAL_EPOCH = 1 #We choose the ideal epoch (thanks to the plot above)\n",
    "\n",
    "############################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#EVALUATING THE MODEL\n",
    "\n",
    "# - First we have to train again the model with the whole training set (train + validation)\n",
    "\n",
    "x_train_val = np.concatenate((x_train, x_val), axis=0)\n",
    "y_train_val = np.concatenate((y_train, y_val), axis=0)\n",
    "\n",
    "if WITH_DATA_AUGMENTATION:\n",
    "    train_generator = data_augmentation.flow(x_train_val, y_train_val, batch_size=BATCH_SIZE)\n",
    "    steps_per_epoch = train_generator.n // train_generator.batch_size\n",
    "    history = model.fit(train_generator, epochs=IDEAL_EPOCH, steps_per_epoch=steps_per_epoch)\n",
    "else:\n",
    "    history = model.fit(x_train_val, y_train_val, batch_size=BATCH_SIZE, epochs=IDEAL_EPOCH, verbose=0)\n",
    "\n",
    "\n",
    "# - Then we evaluate the model on the test set\n",
    "\n",
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CONFUSION MATRIX\n",
    "\n",
    "GENDER = [\"Woman\", \"Man\"]\n",
    "\n",
    "#We compute the predictions\n",
    "y_pred = model.predict(x_test)\n",
    "y_pred = np.argmax(y_pred, axis=1)\n",
    "\n",
    "#We store the true labels\n",
    "y_true = np.argmax(y_test, axis=1)\n",
    "\n",
    "#We compute the confusion matrix\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "#We plot the confusion matrix\n",
    "df_cm = pd.DataFrame(cm, index = [i for i in GENDER],\n",
    "                    columns = [i for i in GENDER])\n",
    "plt.figure(figsize = (10,7))\n",
    "sns.heatmap(df_cm, annot=True, fmt='g')\n",
    "plt.title('Confusion matrix')\n",
    "plt.xlabel('Predicted label')\n",
    "plt.ylabel('True label')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **WE SAVE THE MODEL**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yu7gzQkE3boI"
   },
   "outputs": [],
   "source": [
    "#We save the model\n",
    "\n",
    "############################################################################################################\n",
    "#WE DEFINE THE CONSTANTS\n",
    "name = 'model' #Name of the model\n",
    "\n",
    "############################################################################################################\n",
    "\n",
    "\n",
    "\n",
    "data_augmented = 'data_augmented' if WITH_DATA_AUGMENTATION else ''\n",
    "\n",
    "model.save(name + '_' + data_augmented + '.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **WE LOAD A MODEL**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We load the model\n",
    "\n",
    "############################################################################################################\n",
    "#WE DEFINE THE CONSTANTS\n",
    "file_name = 'model' #Name of the file\n",
    "\n",
    "############################################################################################################\n",
    "\n",
    "model = load_model(file_name + '.h5')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.3 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "570feb405e2e27c949193ac68f46852414290d515b0ba6e5d90d076ed2284471"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
